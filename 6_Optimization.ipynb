{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8RYv0ag8Nyj4gY2bFAeGV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 모델 매개변수 최적화하기\n","\n","* 모델은 학습의 각 반복 단계에서 결과를 추측하고, 추측과 정답 사이의 오류(손실, loss)를 계산하고, 매개변수에 대한 오류의 도함수를 수집한 뒤, 경사하강법을 이용해 파라미터들을 최적화한다.\n"],"metadata":{"id":"ha-xLqkd-5J4"}},{"cell_type":"markdown","source":["## 기본 코드"],"metadata":{"id":"NI7ayYUx_R7m"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"iVvU_X2B-0kF","executionInfo":{"status":"ok","timestamp":1704430809239,"user_tz":-540,"elapsed":9637,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor"]},{"cell_type":"code","source":["training_data = datasets.FashionMNIST(\n","    root = \"data\",\n","    train = True,\n","    download = True,\n","    transform = ToTensor()\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qK7B8BiQ_dk3","executionInfo":{"status":"ok","timestamp":1704430828123,"user_tz":-540,"elapsed":9960,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"1f5f588d-5fa9-426e-c4b3-9682429b5965"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:03<00:00, 8187183.53it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 134820.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:01<00:00, 2543445.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 14738755.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]}]},{"cell_type":"code","source":["test_data = datasets.FashionMNIST(\n","    root = \"data\",\n","    train = False,\n","    download = True,\n","    transform = ToTensor()\n",")"],"metadata":{"id":"lOSAHqHB_kyv","executionInfo":{"status":"ok","timestamp":1704430852066,"user_tz":-540,"elapsed":5,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["train_dataloader = DataLoader(training_data, batch_size = 64)\n","test_dataloader = DataLoader(test_data, batch_size = 64)"],"metadata":{"id":"bynkr_T4_tP3","executionInfo":{"status":"ok","timestamp":1704431391841,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","  def __init__(self):\n","    super(NeuralNetwork, self).__init__()\n","    self.flatten = nn.Flatten()\n","    self.linear_relu_stack = nn.Sequential(\n","        nn.Linear(28*28, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 512),\n","        nn.ReLU(),\n","        nn.Linear(512, 10),\n","    )\n","\n","  def forward(self,x):\n","    x = self.flatten(x)\n","    logits = self.linear_relu_stack(x)\n","    return logits"],"metadata":{"id":"TvNwIoIeBxEu","executionInfo":{"status":"ok","timestamp":1704431517107,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = NeuralNetwork()"],"metadata":{"id":"SgKlGh-oCPqD","executionInfo":{"status":"ok","timestamp":1704431524097,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter\n","* 하이퍼파라미터는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수이다. 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있다.\n","\n","* 학습 시에는 다음과 같은 하이퍼파라미터를 정의한다.\n","  * **에폭(epoch) 수**: 데이터셋을 반복하는 횟수\n","  * **배치 크기 (batch size)**: 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n","  * **학습률(learning rate)** : 각 배치에폭에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있다."],"metadata":{"id":"67v6Q68gCSco"}},{"cell_type":"code","source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"-CZghBP9CRR-","executionInfo":{"status":"ok","timestamp":1704431704526,"user_tz":-540,"elapsed":4,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# 최적화 단계 (Optimization Loop)\n","\n","* 하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있다. 최적화 단계의 각 반복(iteration)을 **에폭**이라고 부른다.\n","\n","* 하나의 에폭은 다음 두 부분으로 구성된다.\n","  1. **학습 단계 (train loop)**: 학습 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴\n","  2. **검증/테스트 단계 (validation/test loop)**: 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)\n"],"metadata":{"id":"LfJhuXsMC-Dh"}},{"cell_type":"markdown","source":["## 1. 학습 단계 (Trainining Loop)\n","### 손실 함수 (Loss function)\n","\n","* **손실함수(loss function)**은 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정하며, 모델은 학습 중에 이 값을 최소화하려고한다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답(label)을 비교해 손실(loss)를 계산\n","\n","* 일반적인 손실함수에는 regression task에 사용하는 `nn.MSELoss` (평균 제곱 오차)나 classification에 사용하는 `nn.NLLLoss` (음의 로그 우도), 그리고 `nn.LogSoftmax`와 `nn.NLLLoss`를 합친 `nn.CrossEntropyLoss` 등이 있다."],"metadata":{"id":"O7DXN5JmEUku"}},{"cell_type":"code","source":["# 모델의 출력 logit을 nn.CrossEntropyLoss에 전달하여 logit을 정규화하고 예측 오류를 계산해보자.\n","loss_fn = nn.CrossEntropyLoss()\n"],"metadata":{"id":"_oCC5KD4C9Xo","executionInfo":{"status":"ok","timestamp":1704433446041,"user_tz":-540,"elapsed":544,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 옵티마이저 (Optimizer)\n","* 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조절하는 과정\n","* 최적화 알고리즘은 이 과정을 수행하는 방식을 정함 (SGD, GD, ADAM, RMSprop 등 다양한 알고리즘이 있음)\n","* 모든 최적화 절차(Logic)은 `optimizer` 객체에 캡슐화된다.\n","* 학습하려는 모델의 매개변수와 학습률(learning_rate) 하이퍼파라미터를 등록하여 옵티마이저를 초기화\n"],"metadata":{"id":"IkVXQRsQGB7s"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"],"metadata":{"id":"W68Ml_CSGBhL","executionInfo":{"status":"ok","timestamp":1704432873397,"user_tz":-540,"elapsed":5,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Training loop에서 최적화의 단계\n","1. `optimizer.zero_grad()`를 호출하여 모델 매개변수의 경사도를 재설정. 기본적으로 기울기는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정한다.\n","2. `loss.backwards()`를 호출하여 예측 손실(prediction loss)를 역전파한다. PyTorch는 각 매개변수에 대한 손실의 변화도를 저장한다.\n","3. 변화도를 계산한 뒤에는 `optimizer.step()`을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\n"],"metadata":{"id":"W__CiN9zHbwV"}},{"cell_type":"markdown","source":["# 전체 구현\n","* `train_loop`: 최적화 코드 반복 수행\n","* `test_loop`: 테스트 데이터로 모델의 성능 측정"],"metadata":{"id":"ASMZ_ubfIdve"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","  size = len(dataloader.dataset)\n","  for batch, (X, y) in enumerate(dataloader):\n","    # 예측(prediction)과 손실(loss) 계산\n","    pred = model(X)\n","    loss = loss_fn(pred, y)\n","\n","    # 역전파\n","    optimizer.zero_grad() # 모델 매개변수의 경사도 재설정\n","    loss.backward() #loss에 대한 역전파\n","    optimizer.step()\n","\n","    if batch % 100 == 0:\n","      loss, current = loss.item(), (batch + 1) * len(X)\n","      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"],"metadata":{"id":"S6AQpJnZHan-","executionInfo":{"status":"ok","timestamp":1704433771347,"user_tz":-540,"elapsed":3,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def test_loop(dataloader, model, loss_fn):\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  test_loss, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for X,y in dataloader:\n","      pred = model(X)\n","      test_loss += loss_fn(pred, y).item()\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","  test_loss /= num_batches\n","  correct /= size\n","  print(f\"Test error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"],"metadata":{"id":"_Vn2ZQCPK13V","executionInfo":{"status":"ok","timestamp":1704433949434,"user_tz":-540,"elapsed":441,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# 손실 함수와 옵티마이저 초기화 & train_loop와 test_loop에 전달\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","  print(f\"Epoch {t+1}\\n------------------------------\")\n","  train_loop(train_dataloader, model, loss_fn, optimizer)\n","  test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4OREkmMLhYb","executionInfo":{"status":"ok","timestamp":1704434237022,"user_tz":-540,"elapsed":151767,"user":{"displayName":"박유빈학부생","userId":"05269790463358912132"}},"outputId":"ad907735-2f4f-44f3-a8ce-608f51c3f222"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","------------------------------\n","loss: 2.309654 [   64/60000]\n","loss: 2.298455 [ 6464/60000]\n","loss: 2.282496 [12864/60000]\n","loss: 2.267633 [19264/60000]\n","loss: 2.255124 [25664/60000]\n","loss: 2.239504 [32064/60000]\n","loss: 2.226477 [38464/60000]\n","loss: 2.205332 [44864/60000]\n","loss: 2.200482 [51264/60000]\n","loss: 2.162131 [57664/60000]\n","Test error: \n"," Accuracy: 55.1%, Avg loss: 2.164208\n","\n","Epoch 2\n","------------------------------\n","loss: 2.176239 [   64/60000]\n","loss: 2.168624 [ 6464/60000]\n","loss: 2.110530 [12864/60000]\n","loss: 2.123273 [19264/60000]\n","loss: 2.079950 [25664/60000]\n","loss: 2.030264 [32064/60000]\n","loss: 2.045760 [38464/60000]\n","loss: 1.973598 [44864/60000]\n","loss: 1.980638 [51264/60000]\n","loss: 1.908379 [57664/60000]\n","Test error: \n"," Accuracy: 60.5%, Avg loss: 1.905910\n","\n","Epoch 3\n","------------------------------\n","loss: 1.940331 [   64/60000]\n","loss: 1.913430 [ 6464/60000]\n","loss: 1.790803 [12864/60000]\n","loss: 1.829879 [19264/60000]\n","loss: 1.732815 [25664/60000]\n","loss: 1.680261 [32064/60000]\n","loss: 1.697968 [38464/60000]\n","loss: 1.595851 [44864/60000]\n","loss: 1.623858 [51264/60000]\n","loss: 1.514747 [57664/60000]\n","Test error: \n"," Accuracy: 60.4%, Avg loss: 1.532210\n","\n","Epoch 4\n","------------------------------\n","loss: 1.600959 [   64/60000]\n","loss: 1.568095 [ 6464/60000]\n","loss: 1.409065 [12864/60000]\n","loss: 1.482767 [19264/60000]\n","loss: 1.369532 [25664/60000]\n","loss: 1.357555 [32064/60000]\n","loss: 1.370754 [38464/60000]\n","loss: 1.291265 [44864/60000]\n","loss: 1.331024 [51264/60000]\n","loss: 1.224462 [57664/60000]\n","Test error: \n"," Accuracy: 62.8%, Avg loss: 1.255460\n","\n","Epoch 5\n","------------------------------\n","loss: 1.334270 [   64/60000]\n","loss: 1.319020 [ 6464/60000]\n","loss: 1.146245 [12864/60000]\n","loss: 1.255347 [19264/60000]\n","loss: 1.131454 [25664/60000]\n","loss: 1.153237 [32064/60000]\n","loss: 1.174874 [38464/60000]\n","loss: 1.106940 [44864/60000]\n","loss: 1.152434 [51264/60000]\n","loss: 1.062733 [57664/60000]\n","Test error: \n"," Accuracy: 64.5%, Avg loss: 1.087797\n","\n","Epoch 6\n","------------------------------\n","loss: 1.160595 [   64/60000]\n","loss: 1.164663 [ 6464/60000]\n","loss: 0.975843 [12864/60000]\n","loss: 1.117314 [19264/60000]\n","loss: 0.988077 [25664/60000]\n","loss: 1.020687 [32064/60000]\n","loss: 1.058569 [38464/60000]\n","loss: 0.992774 [44864/60000]\n","loss: 1.039174 [51264/60000]\n","loss: 0.965310 [57664/60000]\n","Test error: \n"," Accuracy: 65.7%, Avg loss: 0.981880\n","\n","Epoch 7\n","------------------------------\n","loss: 1.042216 [   64/60000]\n","loss: 1.066423 [ 6464/60000]\n","loss: 0.860941 [12864/60000]\n","loss: 1.026163 [19264/60000]\n","loss: 0.899610 [25664/60000]\n","loss: 0.928534 [32064/60000]\n","loss: 0.983359 [38464/60000]\n","loss: 0.919288 [44864/60000]\n","loss: 0.961532 [51264/60000]\n","loss: 0.900670 [57664/60000]\n","Test error: \n"," Accuracy: 67.1%, Avg loss: 0.910075\n","\n","Epoch 8\n","------------------------------\n","loss: 0.955878 [   64/60000]\n","loss: 0.998751 [ 6464/60000]\n","loss: 0.779075 [12864/60000]\n","loss: 0.961756 [19264/60000]\n","loss: 0.841330 [25664/60000]\n","loss: 0.860969 [32064/60000]\n","loss: 0.930635 [38464/60000]\n","loss: 0.870333 [44864/60000]\n","loss: 0.905926 [51264/60000]\n","loss: 0.854174 [57664/60000]\n","Test error: \n"," Accuracy: 68.4%, Avg loss: 0.858388\n","\n","Epoch 9\n","------------------------------\n","loss: 0.889481 [   64/60000]\n","loss: 0.948208 [ 6464/60000]\n","loss: 0.718092 [12864/60000]\n","loss: 0.913543 [19264/60000]\n","loss: 0.800119 [25664/60000]\n","loss: 0.809694 [32064/60000]\n","loss: 0.890443 [38464/60000]\n","loss: 0.836257 [44864/60000]\n","loss: 0.864097 [51264/60000]\n","loss: 0.818450 [57664/60000]\n","Test error: \n"," Accuracy: 69.6%, Avg loss: 0.819104\n","\n","Epoch 10\n","------------------------------\n","loss: 0.836234 [   64/60000]\n","loss: 0.907709 [ 6464/60000]\n","loss: 0.671080 [12864/60000]\n","loss: 0.875990 [19264/60000]\n","loss: 0.769304 [25664/60000]\n","loss: 0.769976 [32064/60000]\n","loss: 0.857587 [38464/60000]\n","loss: 0.811269 [44864/60000]\n","loss: 0.831346 [51264/60000]\n","loss: 0.789353 [57664/60000]\n","Test error: \n"," Accuracy: 70.8%, Avg loss: 0.787823\n","\n","Done!\n"]}]}]}